{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f42c897-788c-4def-9e51-b703a43df630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c6d4f4-3b8c-4cfd-a88a-41f7af4d129a",
   "metadata": {},
   "source": [
    "## configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52178860-a57f-4e61-bb29-6baf34d7a0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gpt2'\n",
    "dataset = 'cola/raw/in_domain_dev.tsv'\n",
    "number_of_shots = 4\n",
    "prompt_set = '1'\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c80235-d632-4e76-9686-5efb120d49b1",
   "metadata": {},
   "source": [
    "## load the model and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04d566e-d13c-464d-966e-2c0e3cf8db91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForCausalLM.from_pretrained(model, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "df = pd.read_csv(dataset, sep='\\t', names=['source', 'label', 'note', 'sentence'])\n",
    "                \n",
    "inputs_targets_g = prepare_data(df, 'cola', 'g', number_of_shots, prompt_set)\n",
    "inputs_targets_g_prime = prepare_data(df, 'cola', 'g_prime', number_of_shots, prompt_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0d631f-06f6-4eab-a8bd-2874207c09d9",
   "metadata": {},
   "source": [
    "## calculate in-context PVI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcde5173-9ed3-4f24-a7bb-330aa5b74cbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d4fe50-a261-417d-85db-c3ebda138eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.manual_seed_all(2266)\n",
    "raw_responses, results = defaultdict(dict), defaultdict(list)\n",
    "for i in trange(len(df)):\n",
    "    results['input'].append(inputs_targets_g_prime[i][0])\n",
    "    results['target'].append(inputs_targets_g_prime[i][1])\n",
    "    raw_responses['g'][i], scores_g = predict(\n",
    "        inputs_targets_g[i][0],\n",
    "        model,\n",
    "        tokenizer,\n",
    "        max_lengths[model],\n",
    "        max_new_tokens=2,\n",
    "        pad_token_id=pad_ids[model],\n",
    "        eos_token_id=None,\n",
    "        temperature=0.8,\n",
    "        device=device\n",
    "    )\n",
    "    raw_responses['g_prime'][i], scores_g_prime = predict(\n",
    "        inputs_targets_g_prime[i][0],\n",
    "        model,\n",
    "        tokenizer,\n",
    "        max_lengths[model],\n",
    "        max_new_tokens=2,\n",
    "        pad_token_id=pad_ids[model],\n",
    "        eos_token_id=None,\n",
    "        temperature=0.8,\n",
    "        device=device\n",
    "    )\n",
    "    results['scores_g'].append([_.cpu().numpy() for _ in scores_g])\n",
    "    results['scores_g_prime'].append([_.cpu().numpy() for _ in scores_g_prime])\n",
    "    results['prediction_g'].append(raw_responses['g'][i].replace(inputs_targets_g[i][0], ''))\n",
    "    results['prediction_g_prime'].append(raw_responses['g_prime'][i].replace(inputs_targets_g_prime[i][0], ''))\n",
    "\n",
    "    target_id = tokenizer(' ' + results['target'][i]).input_ids[0]\n",
    "    log_softmax_g = scores_g[0].squeeze().log_softmax(dim=0)\n",
    "    log_softmax_g_prime = scores_g_prime[0].squeeze().log_softmax(dim=0)\n",
    "    log_prob_g = float(log_softmax_g[target_id])\n",
    "    log_prob_g_prime = float(log_softmax_g_prime[target_id])\n",
    "    \n",
    "    # replace -inf with the lowest log probability in log_softmax\n",
    "    # if log_prob_g < -100:\n",
    "    #     x = scores_g[0].squeeze().softmax(dim=0)\n",
    "    #     target_id = torch.where(x > 0, x, x.max()).argmin()\n",
    "    #     log_prob_g = float(log_softmax_g[target_id])\n",
    "    # if log_prob_g_prime < -100:\n",
    "    #     x = scores_g_prime[0].squeeze().softmax(dim=0)\n",
    "    #     target_id = torch.where(x > 0, x, x.max()).argmin()\n",
    "    #     log_prob_g_prime = float(log_softmax_g_prime[target_id])\n",
    "    \n",
    "    results['information_g'].append(log_prob_g)\n",
    "    results['information_g_prime'].append(log_prob_g_prime)\n",
    "    results['pvi'].append(-results['information_g'][i] + results['information_g_prime'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4ee015-584c-44c8-9273-4d1bbc65f146",
   "metadata": {},
   "source": [
    "### openai api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b23fc9-6a79-4fbe-a418-6fa921a72399",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.organization = ''\n",
    "openai.api_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd68980-c28b-441a-ab1b-865de2e3f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_responses, results = defaultdict(dict), defaultdict(list)\n",
    "for i in trange(len(df)):\n",
    "    results['input'].append(inputs_targets_g_prime[i][0])\n",
    "    results['target'].append(inputs_targets_g_prime[i][1])\n",
    "    response_g = openai.Completion.create(\n",
    "        prompt=inputs_targets_g[i][0],\n",
    "        model=model,\n",
    "        max_tokens=2,\n",
    "        temperature=0.8,\n",
    "        logprobs=5\n",
    "    )\n",
    "    response_g_prime = openai.Completion.create(\n",
    "        prompt=inputs_targets_g_prime[i][0],\n",
    "        model=model,\n",
    "        max_tokens=2,\n",
    "        temperature=0.8,\n",
    "        logprobs=5\n",
    "    )\n",
    "    raw_responses['g'][i] = response_g\n",
    "    raw_responses['g_prime'][i] = response_g_prime\n",
    "    raw_responses['scores_g'][i] = raw_responses['g'][i]['choices'][0]['logprobs']['top_logprobs'][0]\n",
    "    raw_responses['scores_g_prime'][i] = raw_responses['g_prime'][i]['choices'][0]['logprobs']['top_logprobs'][0]\n",
    "    results['prediction_g'].append(raw_responses['g'][i]['choices'][0]['text'])\n",
    "    results['prediction_g_prime'].append(raw_responses['g_prime'][i]['choices'][0]['text'])\n",
    "\n",
    "    target = ' ' + results['target'][i]\n",
    "    if target in raw_responses['scores_g'][i] and target in raw_responses['scores_g_prime'][i]:\n",
    "        results['information_g'].append(raw_responses['scores_g'][i][target])\n",
    "        results['information_g_prime'].append(raw_responses['scores_g_prime'][i][target])\n",
    "        results['pvi'].append(-results['information_g'][i] + results['information_g_prime'][i])\n",
    "    else:\n",
    "        results['information_g'].append('na')\n",
    "        results['information_g_prime'].append('na')\n",
    "        results['pvi'].append('na')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e993ae9-77d2-4b7c-8d32-ef4824277793",
   "metadata": {},
   "source": [
    "## save the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f07cb4-2a84-4f0d-b5b3-4afe066c5506",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('raw_responses.json', 'w') as file:\n",
    "    json.dump(raw_responses, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "pd.DataFrame(results).to_csv('results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
